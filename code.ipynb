{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iGuNEhV3L9w",
        "colab_type": "text"
      },
      "source": [
        "# Sparse GGNN low-bandwidth model for dense hardware\n",
        "\n",
        "In this notebook we demonstrate the key parts of our implementation of the low-bandwidth graph message propagation algorithm for Graph Neural Networks. This is code accompanying the ICLR 2020 submission **Fast Training of Sparse Graph Neural Networks on Dense Hardware** by Anonymous Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CRZjQbu3jik",
        "colab_type": "text"
      },
      "source": [
        "## The model\n",
        "\n",
        "This section shows the code that is executed on the dense hardware (TPU). We start by showing the code for the crucial inner loop that performs one step of graph message passing. The function, called `build_one_timestep`, corresponds to Algorithm 1 in the paper. Later in this section we show how the `build_one_timestep` function is invoked within the full model, and the next section shows how the input pipeline prepares the data in the format that the model expects.\n",
        "\n",
        "The `build_one_timestep` function assumes that it receives input data as an `EinsumAdjacencyInformation` namedtuple containing Tensors of following shapes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynw6I_464nxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EinsumAdjacencyInformation = collections.namedtuple(\n",
        "    \"EinsumAdjacencyInformation\",\n",
        "    [\"main_diagonal\", \"superior_diagonal\", \"inferior_diagonal\"]\n",
        ")\n",
        "\"\"\"\n",
        "- main_diagonal: [N'/S', S'*P, S] Tensor\n",
        "- superior_diagonal: [N/S-1, S'*P, S] Tensor\n",
        "- inferior_diagonal: [N/S-1, S'*P, S] Tensor\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9dbTnYM4r8I",
        "colab_type": "text"
      },
      "source": [
        "This is the `build_one_timestep` function corresponding to Algorithm 1 in the paper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSUHlvok5u5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_one_timestep(node_states, weights, adjacency_information, biases):\n",
        "  \"\"\"Returns new node states after a single message passing timestep.\n",
        "\n",
        "  Args:\n",
        "    node_states: [N, H] Tensor of previous node states.\n",
        "    weights: Dictionary of weights, as returned by `build_weights`.\n",
        "    adjacency_information: AdjacencyInformation namedtuple containing\n",
        "        adjacency information required for one graph message propagation step.\n",
        "    biases: [N, H] Tensor of precomputed biases to add for each node.\n",
        "  Returns:\n",
        "    new_node_states: [N, H] Tensor of new node states.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract all relevant parameters from self.params.\n",
        "  num_edge_types = params[\"data\"][\"num_edge_types\"]\n",
        "  num_nodes = params[\"data\"][\"num_nodes\"]\n",
        "  block_size = params[\"model\"][\"block_size\"]\n",
        "  assert num_nodes % block_size == 0\n",
        "  num_blocks = num_nodes // block_size\n",
        "  hidden_size = params[\"model\"][\"hidden_size\"]\n",
        "\n",
        "  # Reshape node states in preparation for the message passing einsums.\n",
        "  node_states_for_diagonal = tf.reshape(\n",
        "      node_states,\n",
        "      [num_blocks, block_size, hidden_size]\n",
        "  )  # [N'/S', S, H]\n",
        "\n",
        "  # Message passing einsums.\n",
        "  messages_from_main_diagonal = tf.einsum(\n",
        "      \"kzv,kvh->kzh\",\n",
        "      adjacency_information.main_diagonal,  # [N'/S, S'*P, S]\n",
        "      node_states_for_diagonal  # [N'/S, S, H]\n",
        "  )  # [N'/S, S'*P, H]\n",
        "  messages_from_superior_diagonal = tf.einsum(\n",
        "      \"kzv,kvh->kzh\",\n",
        "      adjacency_information.superior_diagonal,  # [N/S-1, S'*P, S]\n",
        "      node_states_for_diagonal[1:, :, :]  # [N/S-1, S, H]\n",
        "  )  # [N'/S-1, S'*P, H]\n",
        "  messages_from_inferior_diagonal = tf.einsum(\n",
        "      \"kzv,kvh->kzh\",\n",
        "      adjacency_information.inferior_diagonal,  # [N/S-1, S'*P, S]\n",
        "      node_states_for_diagonal[:-1, :, :]  # [N/S-1, S, H]\n",
        "  )  # [N'/S-1, S'*P, H]\n",
        "\n",
        "  # Concat incoming messages with zeros to align the shapes before summing.\n",
        "  zeros = tf.zeros(\n",
        "      [1, block_size * num_edge_types, hidden_size],\n",
        "      dtype=tf.float32\n",
        "  )  # [1, S'*P, H]\n",
        "  messages_from_superior_diagonal_aligned = tf.reshape(\n",
        "      tf.concat([\n",
        "          messages_from_superior_diagonal,\n",
        "          zeros\n",
        "      ], axis=0),  # [N'/S', S'*P, H]\n",
        "      [num_blocks, block_size * num_edge_types, hidden_size]\n",
        "  )  # [N'/S', S'*P, H]\n",
        "  messages_from_inferior_diagonal_aligned = tf.reshape(\n",
        "      tf.concat([\n",
        "          zeros,\n",
        "          messages_from_inferior_diagonal\n",
        "      ], axis=0),  # [N'/S', S'*P, H]\n",
        "      [num_blocks, block_size * num_edge_types, hidden_size]\n",
        "  )  # [N'/S', S'*P, H]\n",
        "\n",
        "  # Aggregate the incoming messages by summing pointwise.\n",
        "  messages = (\n",
        "      messages_from_main_diagonal\n",
        "      + messages_from_superior_diagonal_aligned\n",
        "      + messages_from_inferior_diagonal_aligned\n",
        "  )  # [N'/S, S'*P, H]\n",
        "\n",
        "  # Reshape into [N', P*H'] before returning.\n",
        "  messages = tf.reshape(\n",
        "      messages,\n",
        "      [num_nodes, num_edge_types * hidden_size],\n",
        "      name=\"messages_reshaped\"\n",
        "  )  # [N', P*H']\n",
        "\n",
        "  # Pass incoming messages through a linear layer.\n",
        "  messages_passed = tf.einsum(\n",
        "      \"ny,yh->nh\",  # y=ph\n",
        "      messages,  # [N', P*H]\n",
        "      weights[\"edge_weights\"],  # [P*H, H]\n",
        "      name=\"messages_passed\"\n",
        "  )  # [N', H]\n",
        "\n",
        "  # Add the edge biases.\n",
        "  messages_passed += biases\n",
        "\n",
        "  # Update `new_node_states` using the built recurrent unit.\n",
        "  new_node_states = weights[\"rnn_cell\"](\n",
        "      incoming_information,\n",
        "      node_states\n",
        "  )[1]  # [N, H]\n",
        "  return new_node_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZRv9jAu66k6",
        "colab_type": "text"
      },
      "source": [
        "The `build_one_timestep` function referred to a dictionary of `weights`, which contains all the trainable parameters of the model. They are build in the following `build_weights` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVKanpzL7Fgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_weights(self, hidden_size, num_edge_types, dropout_keep_prob):\n",
        "  \"\"\"Builds and returns weights used by the GGNN propagation model.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of weights mapping from weight names to lists of Tensors.\n",
        "  \"\"\"\n",
        "\n",
        "  weights = {}\n",
        "\n",
        "  # Build edge weights.\n",
        "  weights[\"edge_weights\"] = tf.get_variable(\n",
        "        \"gnn_edge_weights\",\n",
        "        shape=[num_edge_types * hidden_size, hidden_size],\n",
        "        initializer=tf.contrib.layers.xavier_initializer()\n",
        "  )  # [P*H, H]\n",
        "\n",
        "  # Build edge biases.\n",
        "  weights[\"edge_biases\"] = tf.get_variable(\n",
        "        \"gnn_edge_biases\", shape=[num_edge_types, hidden_size]\n",
        "  )  # [P, H]\n",
        "\n",
        "  # Build RNN cell.\n",
        "  cell = tf.nn.rnn_cell.GRUCell(hidden_size, activation=activation_fun)\n",
        "  if self.mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    cell = tf.nn.rnn_cell.DropoutWrapper(\n",
        "        cell, state_keep_prob=dropout_keep_prob)\n",
        "  weights[\"rnn_cells\"] = cell\n",
        "\n",
        "  # Build dense output layer weights.\n",
        "  weights[\"output_weights\"] = tf.get_variable(\n",
        "      \"output_weights\",\n",
        "      shape=[hidden_size, 1],\n",
        "      initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "  return weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmRDW8ED8c5B",
        "colab_type": "text"
      },
      "source": [
        "The function that performs the loop across $T$ graph message passing propagation steps and thus produces the final node embeddings, is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2as5cQJ8lA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_final_node_representations(\n",
        "    weights, initial_node_representation, adjacency_information,\n",
        "    num_incoming_edges_per_type):\n",
        "  \"\"\"Builds graph message passing computation.\n",
        "\n",
        "  Args:\n",
        "    weights: Dictionary of weights, as constructed by `build_weights`.\n",
        "    initial_node_representation: Tensor of initial node states from the input\n",
        "        pipeline, in a shape that can be reshaped to [N, H].\n",
        "    adjacency_information: A namedtuple with the adjacency information\n",
        "        required to compute incoming messages using `compute_messages`.\n",
        "    num_incoming_edges_per_type: [P, N] Tensor of per-type inedge counts.\n",
        "  Returns:\n",
        "    [N, H] Tensor with node embeddings after graph propagation.\n",
        "  \"\"\"\n",
        "\n",
        "  # Edge biases don't depend on the timestep, and so can be precomputed.\n",
        "  biases = tf.einsum(\n",
        "      \"np,ph->nh\",\n",
        "      num_incoming_edges_per_type,  # [N', P]\n",
        "      weights[\"edge_biases\"],  # [P, H]\n",
        "      name=\"biases\"\n",
        "  )  # [N', H]\n",
        "\n",
        "  # Perform the number T of propagation steps specified.\n",
        "  num_timesteps = params[\"model\"][\"num_timesteps\"]\n",
        "  node_states = initial_node_representation\n",
        "  for step in range(num_timesteps):\n",
        "    node_states = self.build_one_timestep(\n",
        "        node_states, weights, adjacency_information, biases)\n",
        "\n",
        "  return node_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FP7tT7J9XsN",
        "colab_type": "text"
      },
      "source": [
        "This concludes our implementation of a GGNN encoder with low-bandwidth graph message passing. The output of the GGNN encoder is then fed into a readout layer that computes the predictions and the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi4_zGda9VDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(self, features, labels):\n",
        "  \"\"\"Builds the part of the model that is to be run on a TPU.\"\"\"\n",
        "\n",
        "  num_nodes = params[\"data\"][\"num_nodes\"]\n",
        "  hidden_size = params[\"model\"][\"hidden_size\"]\n",
        "\n",
        "  weights = self.build_weights()\n",
        "  adjacency_information, num_incoming_edges_per_type = (\n",
        "      self.prepare_adjacency_information(features))\n",
        "  final_node_representations = self.compute_final_node_representations(\n",
        "      weights,\n",
        "      tf.reshape(features[\"initial_node_states\"], [num_nodes, hidden_size]),\n",
        "      adjacency_information,\n",
        "      num_incoming_edges_per_type\n",
        "  )\n",
        "\n",
        "  # Map from final node representations to predicted labels.\n",
        "  node_logits = tf.squeeze(\n",
        "      tf.einsum(\n",
        "          \"nh,hi->ni\",\n",
        "          final_node_representations,\n",
        "          weights[\"output_weights\"]),\n",
        "      axis=-1,\n",
        "      name=\"node_logits\"\n",
        "  )  # [N]\n",
        "  loss = self.build_loss(\n",
        "      node_logits,\n",
        "      features[\"candidates\"],\n",
        "      labels\n",
        "  )\n",
        "  return loss\n",
        "\n",
        "def build_loss(self,\n",
        "               node_logits,\n",
        "               candidates,\n",
        "               labels):\n",
        "  \"\"\"Builds the output layer and returns the resulting loss Op.\n",
        "\n",
        "  Args:\n",
        "    node_logits: [N] Tensor of logits (pre-softmax probabilities) that give\n",
        "        the model's predictions of which candidate is correct.\n",
        "    candidates: [N] \"batch hot\" Tensor. A nonzero value indicates whether\n",
        "        each node is a candidate. The value gives the index of the graph (with\n",
        "        indices starting at 1) where the node is a candidate.\n",
        "    labels: [N] binary Tensor indicating whether each node is a correct\n",
        "        solution. Used as the target in computing the loss.\n",
        "  Returns:\n",
        "    A scalar Tensor `loss` containing the value of the loss.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract relevant parameters.\n",
        "  max_graphs_per_supergraph = self.params[\"data\"][\"max_graphs_per_supergraph\"]\n",
        "  label_smoothing = self.params[\"training\"][\"label_smoothing\"]\n",
        "\n",
        "  # Compute binary mask of candidate nodes.\n",
        "  candidates_one_hot = tf.clip_by_value(candidates, 0.0, 1.0)\n",
        "\n",
        "  # Apply softmax within each graph independently.\n",
        "  groups = tf.cast(candidates[0, :], tf.int32)\n",
        "  num_groups = max_graphs_per_supergraph + 1\n",
        "  node_logprobs = log_softmax_by_group(\n",
        "      node_logits[0, :],\n",
        "      groups,\n",
        "      num_groups,\n",
        "  )\n",
        "  node_logprobs = tf.expand_dims(node_logprobs, 0)\n",
        "\n",
        "  # Apply label smoothing.\n",
        "  labels_smoothed = smooth_by_group(\n",
        "      labels[0, :], groups, num_groups, label_smoothing)\n",
        "  # Remove label smoothing from group 0 (non-candidates).\n",
        "  labels_smoothed = tf.multiply(candidates_one_hot, labels_smoothed)\n",
        "\n",
        "  losses_per_node = tf.multiply(labels_smoothed, -node_logprobs)  # [N]\n",
        "  loss_normalizer = tf.reduce_sum(labels)\n",
        "\n",
        "  # Mask out losses of non-candidate nodes.\n",
        "  losses_masked = tf.multiply(candidates_one_hot, losses_per_node)  # [N]\n",
        "\n",
        "  # Compute average loss across both batch and superbatch.\n",
        "  loss = tf.reduce_sum(losses_masked) / loss_normalizer\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT1KHcPABHRl",
        "colab_type": "text"
      },
      "source": [
        "The `build_model` function calls `prepare_adjacency_information`, which is a helper function that converts a dictionary of `features` passed to the dense hardware (TPU) by the input pipeline (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIQ4sJ7kBRV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_adjacency_information(features):\n",
        "  \"\"\"Builds Ops preprocessing features for graph propagation.\n",
        "\n",
        "    Args:\n",
        "      features: Dictionary of feature tensors from the input pipeline.\n",
        "    Returns:\n",
        "      adjacency_information: AdjacencyInformation namedtuple required for one\n",
        "          graph message propagation step.\n",
        "      num_incoming_edges_per_type: [P, N] Tensor of incoming edge counts per\n",
        "          edge type (P) and node (N). This Tensor is used in computing the edge\n",
        "          bias terms in graph message propagation.\n",
        "    \"\"\"\n",
        "\n",
        "  # Extract all relevant parameters from self.params.\n",
        "  num_edge_types = self.params[\"data\"][\"num_edge_types\"]\n",
        "  num_nodes = self.params[\"data\"][\"num_nodes\"]\n",
        "  block_size = self.params[\"model\"][\"block_size\"]\n",
        "  assert num_nodes % block_size == 0\n",
        "  num_blocks = num_nodes // block_size\n",
        "  transfer_dtype = self.params[\"input_pipeline\"][\"transfer_dtype\"]\n",
        "\n",
        "  # Extract relevant features.\n",
        "  main_diagonal = features[\"main_diagonal\"]\n",
        "  superior_diagonal = features[\"superior_diagonal\"]\n",
        "  inferior_diagonal = features[\"inferior_diagonal\"]\n",
        "  num_inedges_per_type = features[\"num_incoming_edges_per_type\"]\n",
        "\n",
        "  # Unflatten the features received from the input pipeline.\n",
        "  main_diagonal = tf.reshape(\n",
        "      main_diagonal,\n",
        "      [num_blocks, block_size * num_edge_types, block_size]\n",
        "  )  # [N'/S', S'*T, S]\n",
        "  superior_diagonal = tf.reshape(\n",
        "      superior_diagonal,\n",
        "      [num_blocks - 1, block_size * num_edge_types, block_size]\n",
        "  )  # [N/S-1, S'*T, S]\n",
        "  inferior_diagonal = tf.reshape(\n",
        "      inferior_diagonal,\n",
        "      [num_blocks - 1, block_size * num_edge_types, block_size]\n",
        "  )  # [N/S-1, S'*T, S]\n",
        "  num_inedges_per_type = tf.reshape(\n",
        "      num_inedges_per_type,\n",
        "      [num_nodes, num_edge_types]\n",
        "  )  # [N, P]\n",
        "\n",
        "  # Cast back to tf.float32 if necessary.\n",
        "  if transfer_dtype != tf.float32:\n",
        "    main_diagonal = tf.cast(main_diagonal, tf.float32)\n",
        "    superior_diagonal = tf.cast(superior_diagonal, tf.float32)\n",
        "    inferior_diagonal = tf.cast(inferior_diagonal, tf.float32)\n",
        "    num_inedges_per_type = tf.cast(num_inedges_per_type, tf.float32)\n",
        "\n",
        "  adjacency_information = EinsumAdjacencyInformation(\n",
        "      main_diagonal=main_diagonal,\n",
        "      superior_diagonal=superior_diagonal,\n",
        "      inferior_diagonal=inferior_diagonal\n",
        "  )\n",
        "  return adjacency_information, num_inedges_per_type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvXcVEPG_BHR",
        "colab_type": "text"
      },
      "source": [
        "The `build_loss` function made reference to two following two helper functions, which are used to respectively apply softmax and smoothing on a per-graph basis within a supergraph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7HhnplO-9uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax_by_group(node_scores, groups, num_groups):\n",
        "  \"\"\"Compute per-graph log-softmaxes without any per-graph batching.\n",
        "\n",
        "  Args:\n",
        "    node_scores: A num_nodes tensor of per-node scores.\n",
        "    groups: A num_nodes tensor of normalizing group identities.\n",
        "    num_groups: Upper bound on the max value in `groups` + 1.\n",
        "  Returns:\n",
        "    A tensor the same shape as node_scores with scores per group normalized so\n",
        "    so that exponentiated results sum to 1 within each group.\n",
        "  \"\"\"\n",
        "\n",
        "  # Subtract per-group maxes for numerical stability.\n",
        "  group_maxes = tf.unsorted_segment_max(node_scores, groups, num_groups)\n",
        "  stablizers = tf.gather(group_maxes, groups)\n",
        "  stablized_scores = node_scores - stablizers\n",
        "\n",
        "  # Compute per-group normalizing constants after subtracting stablizers.\n",
        "  group_sum_exps = tf.unsorted_segment_sum(tf.exp(stablized_scores),\n",
        "                                           groups,\n",
        "                                           num_groups)\n",
        "  normalizers = tf.gather(group_sum_exps, groups)\n",
        "\n",
        "  # Normalize scores to get logprobs.\n",
        "  node_logprobs = stablized_scores - tf.log(normalizers)\n",
        "  return node_logprobs\n",
        "\n",
        "def smooth_by_group(labels, groups, num_groups, smoothing):\n",
        "  \"\"\"Applies label smoothing to `labels`, per group.\n",
        "\n",
        "  Args:\n",
        "    labels: A binary tensor.\n",
        "    groups: A non-negative integer tensor of the same shape as `labels`.\n",
        "    num_groups: An upper bound on the integers in `groups`, plus one.\n",
        "    smoothing: A float in [0, 1] parametrising the strength of smoothing.\n",
        "  Returns:\n",
        "    Tensor of floats in [0, 1] of the same shape as `labels`, obtained by\n",
        "    applying label smoothing [Szegedy et al., 2016] to each group individually.\n",
        "  \"\"\"\n",
        "\n",
        "  group_sizes_gathered = tf.gather(\n",
        "      tf.unsorted_segment_sum(tf.ones_like(labels), groups, num_groups),\n",
        "      groups\n",
        "  )\n",
        "  on_value = 1.0 - smoothing\n",
        "  off_values = tf.divide(smoothing, tf.maximum(1.0, group_sizes_gathered - 1))\n",
        "  labels_smoothed = tf.add(labels * (on_value - off_values), off_values)\n",
        "  return labels_smoothed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiNfgP-WvDkG",
        "colab_type": "text"
      },
      "source": [
        "## The input pipeline\n",
        "\n",
        "The crucial part of the input pipeline that is specific to our low-bandwidth model is the following `_compute_einsum_representation` function, which takes as input a sparse representation of the non-zero indices in the adjacency matrices (see docstring in the code below), and computes the densified block diagonals to be passed to the dense hardware (TPU).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bg5sby31r2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _compute_einsum_representation(params, adjacency_matrix_indices):\n",
        "  \"\"\"Transforms adjacency matrix indices into representation for Einsum.\n",
        "  \n",
        "  Args:\n",
        "    params: Dictionary of parameters.\n",
        "    adjacency_matrix_indices: [B, E, 3] Tensor representing the non-zero indices\n",
        "      of the adjacency matrices. Here B is the superbatch dimension (number of\n",
        "      supergraphs) and E is the maximum number of edges in a supergraph. The\n",
        "      three numbers in the last dimension give the edge type number, source\n",
        "      index and target index of each edge, respectively.\n",
        "  Returns:\n",
        "    Dense Tensors to be passed to the dense hardware.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract all relevant parameters from params.\n",
        "  block_size = params[\"model\"][\"block_size\"]\n",
        "  num_edge_types = params[\"data\"][\"num_edge_types\"]\n",
        "  num_nodes = params[\"data\"][\"num_nodes\"]\n",
        "  max_num_edges = params[\"data\"][\"max_num_edges\"]\n",
        "  superbatch_size = params[\"batch_size\"]\n",
        "  transfer_dtype = params[\"input_pipeline\"][\"transfer_dtype\"]\n",
        "  \n",
        "  # To construct dense adjacency matrices as a Tensor of shape [B, P, N, N'],\n",
        "  # we first need to transform the [B, E, 3] `adjacency_matrix_indices` into\n",
        "  # an `indices` Tensor of shape [B*E, 4], with the new first column (in the\n",
        "  # last dimension) containing the superbatch index.\n",
        "  indices_with_superbatch = prepend_first_coordinate_to_third_dimension(\n",
        "      adjacency_matrix_indices, superbatch_size, max_num_edges)  # [B, E, 4]\n",
        "  indices = tf.reshape(indices_with_superbatch, [-1, 4])  # [B*E, 4]\n",
        "\n",
        "  # Filter out padding\n",
        "  indices = tf.boolean_mask(\n",
        "      indices,\n",
        "      tf.greater_equal(indices[:, 1], 0)\n",
        "  )  # [B*E, 4]\n",
        "\n",
        "  target_block_indices = tf.floor_div(indices[:, 3], block_size)\n",
        "  source_block_indices = tf.floor_div(indices[:, 2], block_size)\n",
        "\n",
        "  # Distribute the edge lists according to whether each edge lies on the\n",
        "  # main (block) diagonal, the superior (block) diagional, or the inferior\n",
        "  # (block) diagonal in the *TRANSPOSED* adjacency matrix.\n",
        "  main_diagonal_indices = tf.boolean_mask(\n",
        "      indices,\n",
        "      tf.equal(target_block_indices, source_block_indices)\n",
        "  )  # [# edges on main block diagonal, 4]\n",
        "  superior_diagonal_indices = tf.boolean_mask(\n",
        "      indices,\n",
        "      tf.equal(target_block_indices + 1, source_block_indices)\n",
        "  )  # [# edges on superior block diagonal, 4]\n",
        "  inferior_diagonal_indices = tf.boolean_mask(\n",
        "      indices,\n",
        "      tf.equal(target_block_indices, source_block_indices + 1)\n",
        "  )  # [num_edges_on_inferior_diagonal, 4]\n",
        "\n",
        "  # Compute incoming edge counts here, as it's simpler to do before the\n",
        "  # splitting up into blocks that happens below.\n",
        "  used_indices = tf.concat([main_diagonal_indices,\n",
        "                            superior_diagonal_indices,\n",
        "                            inferior_diagonal_indices], axis=0)\n",
        "\n",
        "  used_indices_tensor = tf.SparseTensor(\n",
        "      indices=used_indices,\n",
        "      values=tf.ones_like(used_indices[:, 0], dtype=tf.float32),\n",
        "      dense_shape=[superbatch_size, num_edge_types, num_nodes, num_nodes])\n",
        "\n",
        "  # [B, P, N]\n",
        "  num_incoming_edges_by_type = tf.sparse_reduce_sum(used_indices_tensor, 2)\n",
        "  num_incoming_edges_by_type.set_shape([superbatch_size,\n",
        "                                        num_edge_types,\n",
        "                                        num_nodes])\n",
        "  # [B, N, P]\n",
        "  num_incoming_edges_by_type = tf.transpose(num_incoming_edges_by_type,\n",
        "                                            [0, 2, 1])\n",
        "\n",
        "  # Transform the indices such that they refer to the block index and then\n",
        "  # the indices within that block.\n",
        "  main_diagonal_indices = tf.stack([\n",
        "      main_diagonal_indices[:, 0],\n",
        "      tf.floor_div(main_diagonal_indices[:, 2], block_size),\n",
        "      tf.mod(main_diagonal_indices[:, 3], block_size),\n",
        "      main_diagonal_indices[:, 1],\n",
        "      tf.mod(main_diagonal_indices[:, 2], block_size),\n",
        "  ], axis=1)  # [num_edges_on_main_diagonal, 5]\n",
        "  superior_diagonal_indices = tf.stack([\n",
        "      superior_diagonal_indices[:, 0],\n",
        "      tf.floor_div(superior_diagonal_indices[:, 3], block_size),\n",
        "      tf.mod(superior_diagonal_indices[:, 3], block_size),\n",
        "      superior_diagonal_indices[:, 1],\n",
        "      tf.mod(superior_diagonal_indices[:, 2], block_size)\n",
        "  ], axis=1)  # [num_edges_on_superior_diagonal, 5]\n",
        "  inferior_diagonal_indices = tf.stack([\n",
        "      inferior_diagonal_indices[:, 0],\n",
        "      tf.floor_div(inferior_diagonal_indices[:, 2], block_size),\n",
        "      tf.mod(inferior_diagonal_indices[:, 3], block_size),\n",
        "      inferior_diagonal_indices[:, 1],\n",
        "      tf.mod(inferior_diagonal_indices[:, 2], block_size)\n",
        "  ], axis=1)  # [num_edges_on_inferior_diagonal, 5]\n",
        "\n",
        "  # Compute dense representations of the three block diagonals.\n",
        "  main_diagonal = tf.sparse_to_dense(\n",
        "      main_diagonal_indices,\n",
        "      [superbatch_size, num_nodes // block_size, block_size, num_edge_types,\n",
        "        block_size],\n",
        "      tf.ones_like(main_diagonal_indices[:, 0], dtype=transfer_dtype),\n",
        "      validate_indices=False,\n",
        "      name=\"main_diagonal\"\n",
        "  )  # [B, N/S, S', P, S]\n",
        "  superior_diagonal = tf.sparse_to_dense(\n",
        "      superior_diagonal_indices,\n",
        "      [superbatch_size, num_nodes // block_size - 1, block_size,\n",
        "        num_edge_types, block_size],\n",
        "      tf.ones_like(superior_diagonal_indices[:, 0], dtype=transfer_dtype),\n",
        "      validate_indices=False,\n",
        "      name=\"superior_diagonal\"\n",
        "  )  # [B, N/S-1, S', P, S]\n",
        "  inferior_diagonal = tf.sparse_to_dense(\n",
        "      inferior_diagonal_indices,\n",
        "      [superbatch_size, num_nodes // block_size - 1, block_size,\n",
        "        num_edge_types, block_size],\n",
        "      tf.ones_like(inferior_diagonal_indices[:, 0], dtype=transfer_dtype),\n",
        "      validate_indices=False,\n",
        "      name=\"inferior_diagonal\"\n",
        "  )  # [B, N/S-1, S', P, S]\n",
        "\n",
        "  return (main_diagonal,\n",
        "          superior_diagonal,\n",
        "          inferior_diagonal,\n",
        "          num_incoming_edges_by_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVPsF24r23ba",
        "colab_type": "text"
      },
      "source": [
        "The function made reference to a standalone helper function `prepend_first_coordinate_to_third_dimension`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMeI34VP29XO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepend_first_coordinate_to_third_dimension(\n",
        "    tensor, first_dimension_size, second_dimension_size):\n",
        "  \"\"\"Prepends 1st coordinate index as a new column in the 3rd dimension.\n",
        "\n",
        "  Args:\n",
        "    tensor: A 3D Tensor of shape [A, B, C].\n",
        "    first_dimension_size: Value of `A` (size of first dimension of `tensor`).\n",
        "    second_dimension_size: Value of `B` (size of second dimension of `tensor`).\n",
        "  Returns:\n",
        "    Tensor of shape [A, B, C+1], where the (a, b, c) entry equals a if c = 0\n",
        "    and tensor[a, b, c-1] otherwise.\n",
        "  \"\"\"\n",
        "  first_coordinate_range = tf.range(\n",
        "      first_dimension_size,\n",
        "      dtype=tensor.dtype\n",
        "  )  # [A]\n",
        "  first_coordinate_range_reshaped = tf.reshape(\n",
        "      first_coordinate_range,\n",
        "      [first_dimension_size, 1, 1]\n",
        "  )  # [A, 1, 1]\n",
        "  first_coordinates = tf.broadcast_to(\n",
        "      first_coordinate_range_reshaped,\n",
        "      [first_dimension_size, second_dimension_size, 1]\n",
        "  )  # [A, B, 1]\n",
        "  result = tf.concat(\n",
        "      [first_coordinates, tensor],\n",
        "      axis=2\n",
        "  )  # [A, B, C+1]\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtTkcvHK1p-C",
        "colab_type": "text"
      },
      "source": [
        "To show how `_compute_einsum_representation` is invoked, we assume access to a function `_load_tpu_batch_data` that implements the sparse batching logic and returns a padded supergraph. The following `build_input_pipeline` function assembles the `features` dictionary passed over to the dense hardware (TPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTD0ipNEvE1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_pipeline(params, split):\n",
        "  \"\"\"Builds the input pipeline as a TensorFlow computation.\n",
        "\n",
        "  This function can be called in the `input_fn` of a (TPU)Estimator, and the\n",
        "  returned `features` and `labels` (see below) are suitable for being in turn\n",
        "  returned by the `input_fn` itself. The (TPU)Estimator then ensures that these\n",
        "  values are passed to the `model_fn` of the (TPU)Estimator.\n",
        "\n",
        "  Args:\n",
        "    params: Dictionary of parameters from the (TPU)Estimator API's `input_fn`.\n",
        "        Contains four keys:\n",
        "        - \"batch_size\", mapping to size of the superbatch that this input\n",
        "          pipeline is expected to produce. This is a keyword reserved by\n",
        "          TPUEstimator, and it is populated automatically.\n",
        "        - The three keys \"data\", \"model\", and \"training\", all of which map to\n",
        "          dictionaries themselves, with each dictionary mapping from parameter\n",
        "          names to values.\n",
        "    split: A DataSplit enum value.\n",
        "  Returns:\n",
        "    `features` and `labels`, the two values to be returned by the `input_fn`.\n",
        "  \"\"\"\n",
        "\n",
        "  # Retrieve relevant parameters from params.\n",
        "  problem = params[\"data\"][\"problem_name\"]\n",
        "  data_dir = params[\"data\"][\"data_dir\"]\n",
        "  max_bandwidth = params[\"data\"][\"max_bandwidth\"]\n",
        "  num_nodes = params[\"data\"][\"num_nodes\"]\n",
        "  max_num_edges = params[\"data\"][\"max_num_edges\"]\n",
        "  seed = params[\"input_pipeline\"][\"seed\"]\n",
        "  data_loading_config = params[\"input_pipeline\"][\"data_loading\"]\n",
        "\n",
        "  # The `batch_size` entry in `params` is auto-populated by TPUEstimator,\n",
        "  # which ensures that the value is per-host or per-core, as appropriate. See\n",
        "  # https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator\n",
        "  superbatch_size = params[\"batch_size\"]\n",
        "\n",
        "  # Retrieve information about this input pipeline invocation.\n",
        "  if \"context\" in params:\n",
        "    _, invocation_index, total_invocations, _ = (\n",
        "        params[\"context\"].current_input_fn_deployment())\n",
        "  else:\n",
        "    invocation_index = 0\n",
        "    total_invocations = 1\n",
        "\n",
        "  # Retrieve padded TPU data.\n",
        "  holes, candidates, candidate_indices, solutions, edges = (\n",
        "        _load_tpu_batch_data(\n",
        "            problem, data_dir, split, num_nodes, max_bandwidth,\n",
        "            max_num_edges, superbatch_size, invocation_index, total_invocations\n",
        "        )\n",
        "    )\n",
        "\n",
        "  # Ensure static shapes of Tensors constructed from TensorArrays.\n",
        "  holes = tf.reshape(\n",
        "      holes, [superbatch_size, num_nodes], name=\"holes\")\n",
        "  candidates = tf.reshape(\n",
        "      candidates, [superbatch_size, num_nodes], name=\"candidates\")\n",
        "  adjacency_matrix_indices = tf.reshape(\n",
        "      edges, [superbatch_size, max_num_edges, 3],\n",
        "      name=\"adjacency_indices\")\n",
        "  labels = tf.reshape(\n",
        "      solutions, [superbatch_size, num_nodes], name=\"labels\")\n",
        "\n",
        "  # Concatenate node features and pad to hidden size for initial node states.\n",
        "  candidates_one_hot = tf.clip_by_value(candidates, 0.0, 1.0)\n",
        "  holes_one_hot = tf.clip_by_value(holes, 0.0, 1.0)\n",
        "  node_annotations_list = [\n",
        "      tf.expand_dims(holes_one_hot, axis=2),  # [superbatch_size, N, 1]\n",
        "      tf.expand_dims(candidates_one_hot, axis=2),  # [superbatch_size, N, 1]\n",
        "  ]\n",
        "  node_annotations = tf.concat(node_annotations_list, axis=2)\n",
        "  annotation_size = node_annotations.shape[2]\n",
        "  assert hidden_size >= annotation_size\n",
        "  initial_node_states = tf.pad(\n",
        "      node_annotations,  # [superbatch_size, N, annotation_size]\n",
        "      [[0, 0], [0, 0], [0, hidden_size - annotation_size]],\n",
        "      name=\"initial_node_states\"\n",
        "  )  # [superbatch_size, N, H]\n",
        "\n",
        "  # Convert adjacency information to format suitable for the einsum.\n",
        "  (main_diagonal,\n",
        "    superior_diagonal,\n",
        "    inferior_diagonal,\n",
        "    num_incoming_edges_per_type) = _compute_einsum_representation(\n",
        "        params, adjacency_matrix_indices)\n",
        "\n",
        "  # Explicitly flatten the large Tensors being sent to TPU.\n",
        "  # If superbatch_size > 1, this dimension should be preserved for sharding.\n",
        "  superbatch_size = params[\"batch_size\"]\n",
        "  target_shape = [-1] if superbatch_size == 1 else [superbatch_size, -1]\n",
        "  initial_node_states = tf.reshape(\n",
        "      initial_node_states, target_shape)\n",
        "  main_diagonal = tf.reshape(main_diagonal, target_shape)\n",
        "  superior_diagonal = tf.reshape(superior_diagonal, target_shape)\n",
        "  inferior_diagonal = tf.reshape(inferior_diagonal, target_shape)\n",
        "\n",
        "  # Return `features` and `labels`.\n",
        "  features = {\n",
        "      \"initial_node_states\": initial_node_states,\n",
        "      \"main_diagonal\": main_diagonal,\n",
        "      \"superior_diagonal\": superior_diagonal,\n",
        "      \"inferior_diagonal\": inferior_diagonal,\n",
        "      \"num_incoming_edges_per_type\": num_incoming_edges_per_type,\n",
        "      \"candidates\": candidates,\n",
        "  }\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}